{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69d76ea",
   "metadata": {},
   "source": [
    "\n",
    "# üß™ Interactive Tutorial ‚Äì Build a Tiny Decoder-Only Transformer\n",
    "\n",
    "This notebook is structured as an **interactive lab**:\n",
    "\n",
    "* üî® **Exercise cells** ‚Äì marked *‚úèÔ∏è Exercise* ‚Äì contain TODOs that raise `NotImplementedError`.\n",
    "* üëÄ **Checkpoint cells** ‚Äì run intermediate code to inspect tensors or verify output shapes.\n",
    "\n",
    "Everything uses **pure PyTorch** and runs on CPU-only in Google Colab (GPU makes it faster).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751aa13",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math, textwrap, torch, torch.nn as nn, torch.nn.functional as F\n",
    "torch.manual_seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3a944c",
   "metadata": {},
   "source": [
    "## 1  Prepare a Tiny Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c0944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = textwrap.dedent(\"\"\"\n",
    "  Shall I compare thee to a summer's day?\n",
    "  Thou art more lovely and more temperate.\n",
    "  Rough winds do shake the darling buds of May,\n",
    "  And summer's lease hath all too short a date.\n",
    "\n",
    "  Sometime too hot the eye of heaven shines,\n",
    "  And often is his gold complexion dimm'd;\n",
    "  And every fair from fair sometime declines,\n",
    "  By chance, or nature's changing course, untrimm'd.\n",
    "\n",
    "  But thy eternal summer shall not fade,\n",
    "  Nor lose possession of that fair thou ow'st;\n",
    "  Nor shall Death brag thou wander'st in his shade,\n",
    "  When in eternal lines to time thou grow'st.\n",
    "\n",
    "  The quick brown fox jumps over the lazy dog.\n",
    "  Pack my box with five dozen liquor jugs.\n",
    "  THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG.\n",
    "  PACK MY BOX WITH FIVE DOZEN LIQUOR JUGS.\n",
    "                       \n",
    "\"\"\")\n",
    "print(f'Corpus length: {len(corpus):,} characters')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b67b8e",
   "metadata": {},
   "source": [
    "\n",
    "## 2  Character-Level Tokeniser\n",
    "\n",
    "### ‚úèÔ∏è Exercise 1  \n",
    "Implement the two helper functions:\n",
    "\n",
    "* `encode(text)` ‚Äì returns a list of integer token IDs  \n",
    "* `decode(token_ids)` ‚Äì returns the original string  \n",
    "\n",
    "Run the **checkpoint** cell that follows to verify the round-trip works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b57a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚îÄ‚îÄ Tokeniser skeleton ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "chars = sorted(list(set(corpus)))\n",
    "stoi  = {ch:i for i, ch in enumerate(chars)}\n",
    "itos  = {i:ch for ch,i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "def encode(text: str):\n",
    "    \"\"\"Convert string to list[int] of token IDs.\"\"\"\n",
    "    # TODO: replace NotImplementedError with your code\n",
    "    raise NotImplementedError\n",
    "\n",
    "def decode(token_ids):\n",
    "    \"\"\"Convert list[int] back to string.\"\"\"\n",
    "    # TODO: replace NotImplementedError with your code\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3899cf9",
   "metadata": {},
   "source": [
    "#### üîç Checkpoint 1 ‚Äì Round-trip test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b3c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample = \"Hello\"\n",
    "print(\"Encode:\", encode(sample))\n",
    "print(\"Decode:\", decode(encode(sample)))\n",
    "assert decode(encode(sample)) == sample\n",
    "print(\"‚úîÔ∏è  Round-trip OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8bea4f",
   "metadata": {},
   "source": [
    "### Build Train / Validation Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce485d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = torch.tensor(encode(corpus), dtype=torch.long)\n",
    "split = int(0.9 * len(data))\n",
    "train_data, val_data = data[:split], data[split:]\n",
    "print(f'Train tokens: {len(train_data):,} | Val tokens: {len(val_data):,}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c64c4",
   "metadata": {},
   "source": [
    "\n",
    "### ‚úèÔ∏è Exercise 2  \n",
    "Complete `get_batch` so it returns a batch of input tokens **x** and target\n",
    "tokens **y** (next-token labels). Shapes should be `(batch_size, block_size)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed7eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_batch(split: str, *, block_size=64, batch_size=32):\n",
    "    \"\"\"Return (x,y) each of shape (B, T).\"\"\"\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    src = train_data if split == 'train' else val_data\n",
    "    ix  = torch.randint()#TODO\n",
    "    x   = torch.stack()#TODO\n",
    "    y   = torch.stack()#TODO\n",
    "    return x.to(device), y.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93a9359",
   "metadata": {},
   "source": [
    "#### üîç Checkpoint 2 ‚Äì Batch shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e19da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"x shape:\", xb.shape, \"| y shape:\", yb.shape)\n",
    "assert xb.shape == yb.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a24c8d",
   "metadata": {},
   "source": [
    "\n",
    "## 3  Masked Multi-Head Self-Attention\n",
    "\n",
    "We‚Äôll build a minimal but complete **decoder-style** attention layer.\n",
    "\n",
    "### ‚úèÔ∏è Exercise 3  \n",
    "Fill the missing parts in `SelfAttention.forward`:\n",
    "\n",
    "1. Compute scaled dot-product attention scores  \n",
    "2. Apply the **causal mask** so each token only sees the **past**  \n",
    "3. Apply softmax, and compute the weighted sum of values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, heads: int, block_size: int):\n",
    "        super().__init__()\n",
    "        assert embed_dim % heads == 0, \"embed_dim must divide heads\"\n",
    "        self.heads = heads\n",
    "        self.d  = embed_dim // heads  # d: dimension per head\n",
    "\n",
    "        # Projections: project input embeddings to queries, keys, and values\n",
    "        self.to_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.to_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.to_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)  # Final output projection\n",
    "\n",
    "        # Causal mask: lower-triangular matrix to prevent attending to future tokens\n",
    "        mask = torch.tril(torch.ones(block_size, block_size))\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, C)\n",
    "        # B = batch size\n",
    "        # T = sequence length (block size)\n",
    "        # C = embedding dimension\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Project input x to queries, keys, and values, then reshape for multi-head attention\n",
    "        # After view and transpose: (B, H, T, d)\n",
    "        # H = number of heads, d = head dimension (C // H)\n",
    "        q = self.to_q(x).view(B, T, self.heads, self.d).transpose(1, 2)  # (B, H, T, d)\n",
    "        k = self.to_k(x).view(B, T, self.heads, self.d).transpose(1, 2)  # (B, H, T, d)\n",
    "        v = self.to_v(x).view(B, T, self.heads, self.d).transpose(1, 2)  # (B, H, T, d)\n",
    "\n",
    "        # TODO 1: scaled dot-product\n",
    "        # scores = ... shape (B, H, T, T)\n",
    "        # scores[i, h, t1, t2] = dot product between query at position t1 and key at position t2, for batch i and head h\n",
    "\n",
    "        # TODO 2: apply causal mask (self.mask) before softmax\n",
    "        # The mask is (T, T), broadcasted over batch and heads. It ensures each position can only attend to previous positions (including itself).\n",
    "\n",
    "        # TODO 3: softmax + weight values -> out\n",
    "        # After softmax, multiply attention weights by v to get the output for each head\n",
    "\n",
    "        # Merge heads: transpose back and reshape to (B, T, C)\n",
    "        # out: (B, H, T, d) -> (B, T, H, d) -> (B, T, C)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c2d95",
   "metadata": {},
   "source": [
    "#### üîç Checkpoint 3 ‚Äì Self-Attention sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa9f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tmp = torch.randn(2, 8, 32)  # (B,T,C)\n",
    "att = SelfAttention(embed_dim=32, heads=4, block_size=8)\n",
    "out = att(tmp)\n",
    "print(\"Output shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63234dc6",
   "metadata": {},
   "source": [
    "## 4  Feed-Forward & Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab2622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4*embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*embed_dim, embed_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, heads, block_size):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.sa  = SelfAttention(embed_dim, heads, block_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.ff  = FeedForward(embed_dim)\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907d8e44",
   "metadata": {},
   "source": [
    "\n",
    "### ‚úèÔ∏è Exercise 4  \n",
    "Instantiate a `TransformerBlock` and verify that it **preserves the input shape**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: create random tensor (B=4, T=16, C=64), run through block, assert shapes\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9429eb3",
   "metadata": {},
   "source": [
    "## 5. Building the MiniGPT Model\n",
    "\n",
    "Now that we have all the building blocks (tokenizer, attention, transformer block), let's put them together into a tiny GPT-style language model!  \n",
    "We'll call it `MiniGPT`. This model stacks several Transformer blocks, adds token and position embeddings, and predicts the next character in a sequence.\n",
    "\n",
    "Below is the full model class, with detailed comments and docstrings to help you understand each part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c1b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal GPT-style language model using stacked Transformer blocks.\n",
    "    Args:\n",
    "        vocab_size (int): Number of unique tokens (characters).\n",
    "        embed_dim (int): Embedding dimension for tokens and positions.\n",
    "        heads (int): Number of attention heads per block.\n",
    "        depth (int): Number of Transformer blocks to stack.\n",
    "        block_size (int): Maximum context length (sequence length).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, *, embed_dim=128, heads=4, depth=4, block_size=64):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # Token embedding: maps token IDs to vectors\n",
    "        self.tok_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        # Position embedding: adds information about token position in the sequence\n",
    "        self.pos_emb = nn.Embedding(block_size, embed_dim)\n",
    "\n",
    "        # Stack of Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, heads, block_size)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Final layer normalization and output head\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Custom weight initialization for linear layers.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass for MiniGPT.\n",
    "        Args:\n",
    "            idx (Tensor): Input token IDs, shape (batch, seq_len)\n",
    "            targets (Tensor, optional): Target token IDs for loss computation.\n",
    "        Returns:\n",
    "            logits (Tensor): Raw predictions for each token.\n",
    "            loss (Tensor or None): Cross-entropy loss if targets are provided.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = idx.shape\n",
    "        assert seq_len <= self.block_size, \"Input sequence too long!\"\n",
    "\n",
    "        # Get token and position embeddings\n",
    "        token_embeddings = self.tok_emb(idx)  # (B, T, C)\n",
    "        position_ids = torch.arange(seq_len, device=idx.device)\n",
    "        position_embeddings = self.pos_emb(position_ids)  # (T, C)\n",
    "        x = token_embeddings + position_embeddings  # (B, T, C)\n",
    "\n",
    "        # Pass through each Transformer block\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Final normalization and output projection\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        # Optionally compute loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens=50, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate new tokens from a prompt.\n",
    "        Args:\n",
    "            idx (Tensor): Initial context tokens, shape (batch, seq_len)\n",
    "            max_new_tokens (int): How many tokens to generate.\n",
    "            temperature (float): Sampling temperature.\n",
    "        Returns:\n",
    "            idx (Tensor): The extended sequence including generated tokens.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]  # Crop to block size\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1] / temperature  # Focus on last token\n",
    "            probs = logits.softmax(dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2045f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### üìù Your Turn: Instantiate and Inspect MiniGPT\n",
    "\n",
    "Let's create a MiniGPT model with just 2 Transformer blocks (for speed!) and check how many parameters it has. \n",
    "\n",
    "**Goal:** Make sure the total parameter count is less than 1 million.\n",
    "\n",
    "#### Hint: Use `sum(p.numel() for p in model.parameters())` to count parameters.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e46ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: instantiate model with vocab_size and depth=2, then print parameter count\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5492e",
   "metadata": {},
   "source": [
    "If you see \"Total parameters: ...\" and it's less than 1,000,000, you're good to go! This compact model will train quickly while still demonstrating all the key Transformer concepts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6798b5ac",
   "metadata": {},
   "source": [
    "---\n",
    "üéâ **Congratulations!** You've successfully built a complete MiniGPT model from scratch! \n",
    "\n",
    "Next, let's test it without training and then we can train it on our Shakespeare corpus and see what kind of text it can generate.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b106c211",
   "metadata": {},
   "source": [
    "\n",
    "### ‚úèÔ∏è Exercise 6  \n",
    "Generate **100** new characters starting from a newline token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: use model.generate to produce text and decode it\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de70ae",
   "metadata": {},
   "source": [
    "## 6  Quick Training Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8534de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for step in range(1, 1501):\n",
    "    xb, yb = get_batch('train')\n",
    "    _, loss = model(xb, yb)\n",
    "    opt.zero_grad(); loss.backward(); opt.step()\n",
    "    if step % 50 == 0:\n",
    "        print(f\"step {step:3d} | loss {loss.item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2096c0c1",
   "metadata": {},
   "source": [
    "\n",
    "## 7  Next Steps  \n",
    "* Try changing the number of training steps or the learning rate and see how it affects the generated text.\n",
    "* Modify the prompt you use for generation and observe how the output changes.\n",
    "* Experiment with the model's context length (block_size) and see what happens.\n",
    "* Look at the model's parameters (e.g., number of layers or heads) and try small adjustments.\n",
    "* Think about what kinds of mistakes the model makes and why.\n",
    "Happy experimenting! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03bc2d3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlbootcamp2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
